{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_directory = 'movie_dataset_multilabel/images/'\n",
    "df = pd.read_csv('movie_dataset_multilabel/movie_metadata.csv')    \n",
    "df = df.iloc[:2000]  # Using the same subset of data for memory reasons.\n",
    "\n",
    "# Image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((200, 200)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dataset = []\n",
    "for i in tqdm(range(df.shape[0])):\n",
    "    img = Image.open(image_directory + df['Id'][i] + '.jpg')\n",
    "    img = transform(img)\n",
    "    X_dataset.append(img)\n",
    "\n",
    "X = torch.stack(X_dataset)\n",
    "y = torch.tensor(np.array(df.drop(['Id', 'Genre'], axis=1)), dtype=torch.float32)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=20, test_size=0.3)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiLabelNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, activation=F.relu)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, activation=F.relu)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=5, activation=F.relu)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=5, activation=F.relu)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        self.fc1 = nn.Linear(64 * 10 * 10, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 25)  # Assuming 25 genres as in Keras example\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(self.bn1(self.conv1(x)), 2)\n",
    "        x = F.dropout(x, 0.2)\n",
    "        x = F.max_pool2d(self.bn2(self.conv2(x)), 2)\n",
    "        x = F.dropout(x, 0.2)\n",
    "        x = F.max_pool2d(self.bn3(self.conv3(x)), 2)\n",
    "        x = F.dropout(x, 0.2)\n",
    "        x = F.max_pool2d(self.bn4(self.conv4(x)), 2)\n",
    "        x = F.dropout(x, 0.2)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, 0.5)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, 0.5)\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiLabelNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for inputs, targets in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += targets.size(0)\n",
    "                correct += (predicted == targets).sum().item()\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Accuracy: {100 * correct / total}%')\n",
    "torch.save(model.state_dict(), 'movie_genre_classifier.pth')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_single_image(image_path, model, transform):\n",
    "    img = Image.open(image_path)\n",
    "    img = transform(img)\n",
    "    img = img.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(img)\n",
    "        probabilities = torch.sigmoid(outputs).squeeze(0)\n",
    "        top_prob, top_catid = torch.topk(probabilities, 10)\n",
    "        for i in range(top_prob.size(0)):\n",
    "            print(df.columns[2:][top_catid[i]], top_prob[i].item())\n",
    "\n",
    "evaluate_single_image('ddlj.jpg', model, transform)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
